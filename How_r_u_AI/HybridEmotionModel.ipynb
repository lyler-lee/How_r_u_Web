{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# 1) EmotionCNN 정의\n",
    "class EmotionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1   = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2   = nn.Conv2d(32,64, 3, padding=1)\n",
    "        self.pool    = nn.MaxPool2d(2,2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # [B,32,24,24]\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # [B,64,12,12]\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)             # flatten → [B, 64*12*12]\n",
    "        return x\n",
    "\n",
    "# 2) HybridEmotionModel 정의 (EmotionCNN + ResNet18 결합)\n",
    "class HybridEmotionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # (1) custom CNN 백본\n",
    "        self.cnn = EmotionCNN()\n",
    "\n",
    "        # (2) ResNet18 백본 (흑백 입력, 분류 레이어 제거)\n",
    "        self.resnet = resnet18(weights=None)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64,\n",
    "                                     kernel_size=7,\n",
    "                                     stride=2,\n",
    "                                     padding=3,\n",
    "                                     bias=False)\n",
    "        self.resnet.fc = nn.Identity()  # 최종 FC를 제거하고 feature만 반환\n",
    "\n",
    "        # (3) 두 feature를 합친 뒤 감정 분류기\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear((64*12*12) + 512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 7)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x1: EmotionCNN → [B, 9216],  x2: ResNet18 → [B, 512]\n",
    "        x1 = self.cnn(x)\n",
    "        x2 = self.resnet(x)\n",
    "        x  = torch.cat((x1, x2), dim=1)  # [B, 9728]\n",
    "        return self.fc(x)\n",
    "\n",
    "# —— 위 정의 셀을 실행하신 뒤에는 아래 코드만 실행하세요 ——\n",
    "\n",
    "# 3) Device 설정 (MPS 전용 예시)\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# 4) 모델 생성 및 MPS 이동\n",
    "model = HybridEmotionModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1️⃣ 데이터 증강 (훈련용)\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomResizedCrop(48, scale=(0.7, 1.0), ratio=(0.9, 1.1)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# 2️⃣ 검증용 전처리\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# 1) FER2013 로더\n",
    "fer_train = ImageFolder('fer2013/train', transform=train_tf)\n",
    "fer_val   = ImageFolder('fer2013/test',   transform=val_tf)\n",
    "fer_train_loader = DataLoader(fer_train, batch_size=128, shuffle=True)\n",
    "fer_val_loader   = DataLoader(fer_val, batch_size=128)\n",
    "\n",
    "# 2) optimizer & criterion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500  # 원하는 만큼 조정 가능\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in fer_train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_train_loss = train_loss / len(fer_train_loader.dataset)\n",
    "    train_acc = 100. * correct / total\n",
    "\n",
    "    # —— Validation —— #\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    emotion_distribution = torch.zeros(7)  # FER2013은 7개 클래스\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in fer_val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            for p in preds:\n",
    "                emotion_distribution[p.item()] += 1\n",
    "\n",
    "    avg_val_loss = val_loss / len(fer_val_loader.dataset)\n",
    "    val_acc = 100. * correct / total\n",
    "\n",
    "    # —— 감정별 예측 분포 출력 —— #\n",
    "    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    dist_str = \", \".join([\n",
    "        f\"{label}: {int(count)}\" for label, count in zip(emotion_labels, emotion_distribution)\n",
    "    ])\n",
    "\n",
    "    print(f\"[에폭 {epoch}/{num_epochs}] \"\n",
    "          f\"훈련 손실: {avg_train_loss:.4f} | 훈련 정확도: {train_acc:.2f}% || \"\n",
    "          f\"검증 손실: {avg_val_loss:.4f} | 검증 정확도: {val_acc:.2f}%\")\n",
    "    print(f\"→ 예측 분포: {dist_str}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"fer_pretrained.pth\")\n",
    "print(\"✅ FER2013 기반 모델이 fer_pretrained.pth로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "# 1️⃣ 데이터 증강 (훈련용)\n",
    "# 1️⃣ RAF-DB 훈련 데이터 증강\n",
    "raf_train_tf = transforms.Compose([\n",
    "    transforms.Grayscale(),                          # 흑백 (모델 입력 채널에 맞춤)\n",
    "    transforms.RandomHorizontalFlip(p=0.5),          # 좌우 반전\n",
    "    transforms.RandomRotation(20),                   # ±20도 회전\n",
    "    transforms.RandomResizedCrop(48, scale=(0.75, 1.0), ratio=(0.9, 1.1)),  # 크롭 + 확대 축소\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # 밝기 & 대비 변화\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# 2️⃣ RAF-DB 검증 전처리\n",
    "raf_val_tf = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "# 1) RAF-DB 로더\n",
    "raf_train = ImageFolder('RAF_DB/train', transform=raf_train_tf)\n",
    "raf_val   = ImageFolder('RAF_DB/test',   transform=raf_val_tf)\n",
    "raf_train_loader = DataLoader(raf_train, batch_size=128, shuffle=True)\n",
    "raf_val_loader   = DataLoader(raf_val, batch_size=128)\n",
    "\n",
    "# 3) Device 설정 (MPS 전용 예시)\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# 4) 모델 생성 및 MPS 이동\n",
    "model = HybridEmotionModel().to(device)\n",
    "\n",
    "# 2) 기존 가중치 불러오기\n",
    "model.load_state_dict(torch.load(\"fer_pretrained.pth\"))\n",
    "\n",
    "# 3) optimizer 재정의 (더 낮은 lr로)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "raf_epochs = 500\n",
    "\n",
    "# 4) 파인튜닝 루프\n",
    "# 4) 파인튜닝 루프\n",
    "for epoch in range(1, raf_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in raf_train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(raf_train_loader.dataset)\n",
    "    train_acc = 100. * correct / total\n",
    "\n",
    "    # ——— 검증 ———\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    emotion_distribution = torch.zeros(7)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in raf_val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "            for p in preds:\n",
    "                emotion_distribution[p.item()] += 1\n",
    "\n",
    "    val_loss /= len(raf_val_loader.dataset)\n",
    "    val_acc = 100. * correct / total\n",
    "\n",
    "    # 감정별 분포 출력\n",
    "    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    dist_str = \", \".join([\n",
    "        f\"{label}: {int(cnt)}\" for label, cnt in zip(emotion_labels, emotion_distribution)\n",
    "    ])\n",
    "\n",
    "    print(f\"[에폭 {epoch}/{raf_epochs}] \"\n",
    "          f\"훈련 손실: {train_loss:.4f} | 훈련 정확도: {train_acc:.2f}% || \"\n",
    "          f\"검증 손실: {val_loss:.4f} | 검증 정확도: {val_acc:.2f}%\")\n",
    "    print(f\"→ 예측 분포: {dist_str}\\n\")\n",
    "\n",
    "print(\"✅ RAF-DB 파인튜닝 완료.\")\n",
    "torch.save(model.state_dict(), \"raf_finetuned.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
